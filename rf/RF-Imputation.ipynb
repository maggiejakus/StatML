{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in files\n",
    "#already been split into training people and testing groups\n",
    "#made from impute2\n",
    "\n",
    "testing_SNPs = pd.read_csv('testing.txt', delimiter=' ', header = None)\n",
    "training_SNPs = pd.read_csv('training.txt', delimiter = ' ', header = None)\n",
    "#map22 = pd.read_csv('map22.txt', delimiter = '\\t', header = None)\n",
    "map22_2 = pd.read_csv('chr22.txt', delimiter = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in map\n",
    "#find how many SNPs in our training/testing SNPs (they're the same...) are also in our map\n",
    "\n",
    "#we only have 250 people in our training data\n",
    "#print(map22[:6][0]) #first col is physical position, second is recombination rate with the next SNP, \\\n",
    "#third is genetic map in cM\n",
    "print(training_SNPs.shape) \n",
    "\n",
    "training_SNPs_arr = training_SNPs[:][2].to_numpy()\n",
    "SNPs = int(testing_SNPs.shape[0])\n",
    "\n",
    "#this map doesn't have many cM distances in it\n",
    "#map22_arr_pos = map22.loc[:][0].to_numpy()\n",
    "\n",
    "#this map has more data in it\n",
    "chr22_arr = map22_2.loc[:8000][1].to_numpy() #hard coding to make file smaller (our SNPs all in first 8k)\n",
    "chr22_cM = map22_2.loc[:8000][2].to_numpy()\n",
    "\n",
    "#turns out there are a few duplicates in my chr22 map\n",
    "uniques = np.unique(chr22_arr)\n",
    "print(len(uniques))\n",
    "\n",
    "#find indices in training_SNPs_arr where we have a cM distance\n",
    "indices_in_SNPs = np.where(np.in1d(training_SNPs_arr,chr22_arr))\n",
    "indices_in_map = np.where(np.in1d(chr22_arr, training_SNPs_arr))\n",
    "\n",
    "#format into something nicer\n",
    "idx_map = indices_in_map[0]\n",
    "idx_SNP = indices_in_SNPs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a 2 column matrix\n",
    "#first column = SNP position\n",
    "#second column = cM distance\n",
    "#if no cM distance, put -1 for now\n",
    "\n",
    "cM_map = np.zeros((SNPs,2))\n",
    "for SNP in range(SNPs):\n",
    "    #first column = SNP position\n",
    "    cM_map[SNP, 0] = training_SNPs_arr[SNP]\n",
    "    \n",
    "    #check if the SNP is found by indices_in_SNPs\n",
    "    if SNP in idx_SNP:\n",
    "        #ind = index for which indices_in_SNPs[index] == SNP\n",
    "        ind = np.where(idx_SNP == SNP)\n",
    "        cM_map[SNP, 1] = chr22_cM[idx_map[ind][0]]\n",
    "    else:\n",
    "        #unknowns get -1\n",
    "        cM_map[SNP, 1] = -1\n",
    "        \n",
    "#going to initialize this first value to a cM value of 0\n",
    "cM_map[0][1] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to infer missing cM values\n",
    "#going to just find average value (is this a valid assumption?)\n",
    "\n",
    "for SNP in range(SNPs):\n",
    "    counter = 1\n",
    "    if cM_map[SNP][1] < 0:\n",
    "        i = 0\n",
    "        prev_known_SNP = cM_map[SNP-1][1]\n",
    "        while (cM_map[SNP + i][1] < 0):\n",
    "            counter +=1 \n",
    "            i += 1\n",
    "        next_known_SNP = cM_map[SNP + i][1]\n",
    "        cM_map[SNP][1] = (next_known_SNP - prev_known_SNP)/counter + prev_known_SNP\n",
    "      \n",
    "    \n",
    "cM_dist = cM_map[:, 1] + 1\n",
    "print(cM_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert testing file into 3 part prob dist file\n",
    "#for each SNP, each person, who is 100% AA, AB, or BB, which is written as 00, 01, or 11, gets \n",
    "#turned into one number for AA, one number for AB, one number for BB (so 1, 0,0 as AA, 0, 1, 0 as\n",
    "#AB, and 0,0,1 for BB)\n",
    "\n",
    "people = int((testing_SNPs.shape[1]-5) / 2.0)\n",
    "\n",
    "testing_dist = np.zeros((SNPs, people * 3))\n",
    "maj_homozygous_test = np.zeros((SNPs, people))\n",
    "heterozygous_test = np.zeros((SNPs, people))\n",
    "min_homozygous_test = np.zeros((SNPs, people))\n",
    "\n",
    "for SNP in range(SNPs):\n",
    "    for person in range(people):\n",
    "        if (testing_SNPs.loc[SNP, 5 + person*2] == 0) and (testing_SNPs.loc[SNP, 6 + person*2] == 0):\n",
    "            maj_homozygous_test[SNP, person] = 1\n",
    "        elif (testing_SNPs.loc[SNP, 5 + person*2] == 1) and (testing_SNPs.loc[SNP, 6 + person*2] == 0):\n",
    "            heterozygous_test[SNP, person] = 1\n",
    "        elif (testing_SNPs.loc[SNP, 5 + person*2] == 0) and (testing_SNPs.loc[SNP, 6 + person*2] == 1):\n",
    "            heterozygous_test[SNP, person] = 1\n",
    "        elif (testing_SNPs.loc[SNP, 5 + person*2] == 1) and (testing_SNPs.loc[SNP, 6 + person*2] == 1):\n",
    "            min_homozygous_test[SNP, person] = 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert training file into 3 part prob dist file\n",
    "#for each SNP, each person, who is 100% AA, AB, or BB, which is written as 00, 01, or 11, gets \n",
    "#turned into one number for AA, one number for AB, one number for BB (so 1, 0,0 as AA, 0, 1, 0 as\n",
    "#AB, and 0,0,1 for BB)\n",
    "\n",
    "people_training = int((training_SNPs.shape[1]-5) / 2.0)\n",
    "\n",
    "training_dist = np.zeros((SNPs, people_training * 3))\n",
    "maj_homozygous_train = np.zeros((SNPs, people_training))\n",
    "heterozygous_train = np.zeros((SNPs, people_training))\n",
    "min_homozygous_train = np.zeros((SNPs, people_training))\n",
    "\n",
    "for SNP in range(SNPs):\n",
    "    for person in range(people_training):\n",
    "        if (training_SNPs.loc[SNP, 5 + person*2] == 0) and (training_SNPs.loc[SNP, 6 + person*2] == 0):\n",
    "            maj_homozygous_train[SNP, person] = 1\n",
    "        elif (training_SNPs.loc[SNP, 5 + person*2] == 1) and (training_SNPs.loc[SNP, 6 + person*2]) == 0:\n",
    "            heterozygous_train[SNP, person] = 1\n",
    "        elif (training_SNPs.loc[SNP, 5 + person*2] == 0) and (training_SNPs.loc[SNP, 6 + person*2] == 1):\n",
    "            heterozygous_train[SNP, person] = 1\n",
    "        elif (training_SNPs.loc[SNP, 5 + person*2] == 1) and (training_SNPs.loc[SNP, 6 + person*2] == 1):\n",
    "            min_homozygous_train[SNP, person] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants, based on our data\n",
    "maj_homo_train_T = maj_homozygous_train.transpose()\n",
    "hetero_train_T = heterozygous_train.transpose()\n",
    "min_homo_train_T = min_homozygous_train.transpose()\n",
    "\n",
    "maj_homo_test_T = maj_homozygous_test.transpose()\n",
    "hetero_test_T = heterozygous_test.transpose()\n",
    "min_homo_test_T = min_homozygous_test.transpose()\n",
    "\n",
    "thin_var = 0.1\n",
    "idx_imputing = np.random.choice(int((maj_homo_train_T.shape[1])), int((maj_homo_train_T.shape[1]) * thin_var), replace = False)\n",
    "missing_idx = np.sort(idx_imputing)\n",
    "missing_idx = list(missing_idx)\n",
    "missing_indices = []\n",
    "for x in missing_idx:\n",
    "    if (x < offset):\n",
    "        missing_idx.remove(x)\n",
    "    elif (x > int((maj_homo_train_T.shape[1])) - offset - 1):\n",
    "        missing_idx.remove(x)\n",
    "    else:\n",
    "        missing_indices.append(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS CLEARLY THE GOOD ONE\n",
    "import time\n",
    "def training_faster(offset, thin_var):\n",
    "    t1 = time.time()\n",
    "    np.random.seed(0)\n",
    "\n",
    "    #create indices that will be \"missing\"\n",
    "    idx_imputing = np.random.choice(int((maj_homo_train_T.shape[1])), int((maj_homo_train_T.shape[1]) * thin_var), replace = False)\n",
    "    missing_idx = np.sort(idx_imputing)\n",
    "    missing_idx = list(missing_idx)\n",
    "    missing_indices = []\n",
    "    for x in missing_idx:\n",
    "        if (x < offset):\n",
    "            missing_idx.remove(x)\n",
    "        elif (x > int((maj_homo_train_T.shape[1])) - offset - 1):\n",
    "            missing_idx.remove(x)\n",
    "        else:\n",
    "            missing_indices.append(x)    \n",
    "    kept_indices = [row for row in range(maj_homo_train_T.shape[1]) if row not in missing_indices]\n",
    "\n",
    "    \n",
    "    imputable_maj_train = maj_homo_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "    imputable_min_train = min_homo_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "    imputable_het_train = hetero_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "\n",
    "    imputable_snps = imputable_maj_train[1]\n",
    "    mega_training_data_maj = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "    mega_training_data_min = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "    mega_training_data_het = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "    mega_labels_maj = np.zeros((len(imputable_snps) * people_training))\n",
    "    mega_labels_min = np.zeros((len(imputable_snps) * people_training))\n",
    "    mega_labels_het = np.zeros((len(imputable_snps) * people_training))\n",
    "\n",
    "                \n",
    " \n",
    "  \n",
    "    maj_train_scaled = (((((maj_homo_train_T * 2)+1)*0.5)-1)*2)\n",
    "    min_train_scaled = (((((min_homo_train_T * 2)+1)*0.5)-1)*2)\n",
    "    het_train_scaled = (((((hetero_train_T * 2)+1)*0.5)-1)*2)\n",
    "\n",
    "    #maj_train_scaled = maj_homo_train_T +1\n",
    "    #min_train_scaled = min_homo_train_T +1\n",
    "    #het_train_scaled = hetero_train_T +1    \n",
    "    \n",
    "    maj_train_scaled = maj_train_scaled * cM_dist\n",
    "    min_train_scaled = min_train_scaled * cM_dist\n",
    "    het_train_scaled = het_train_scaled * cM_dist\n",
    "    \n",
    "    correct_size_ones = np.ones((maj_train_scaled.shape[0], 2*offset))\n",
    "    \n",
    "    for SNP in range((maj_homo_train_T.shape[1] - offset*2), (maj_homo_train_T.shape[1] - offset)):\n",
    "    #for SNP in range(offset, (maj_homo_train_T.shape[1] - offset)): #offset to ncol maj_homo_train_T-offset +1\n",
    "        print(SNP)\n",
    "        #the SNP being imputed - the label\n",
    "        test_val_maj = maj_homo_train_T[:,SNP]\n",
    "        test_val_min = min_homo_train_T[:,SNP]\n",
    "        test_val_het = hetero_train_T[:,SNP]\n",
    "\n",
    "        test_val_cM = cM_dist[SNP]\n",
    "        \n",
    "        #calculate the data\n",
    "        below_start_offset = SNP - 1\n",
    "        while below_start_offset not in kept_indices:\n",
    "            below_start_offset -= 1\n",
    "        below_index = kept_indices.index(below_start_offset)\n",
    "\n",
    "        above_start_offset = SNP + 1\n",
    "        while above_start_offset not in kept_indices:\n",
    "            above_start_offset += 1\n",
    "        above_index = kept_indices.index(above_start_offset)\n",
    "        \n",
    "        idx_range = np.r_[kept_indices[(below_index-offset+1):below_index+1],\n",
    "             kept_indices[above_index:above_index+offset]]\n",
    "        idx_range_int = [int(i) for i in idx_range]\n",
    "\n",
    "        train_val_maj = maj_train_scaled[:,idx_range_int]\n",
    "        train_val_min = min_train_scaled[:,idx_range_int]\n",
    "        train_val_het = het_train_scaled[:,idx_range_int]\n",
    "        \n",
    "        #train_val_maj = np.abs(train_val_maj - test_val_cM * correct_size_ones)\n",
    "        #train_val_maj = np.abs(train_val_maj - np.max(train_val_maj))\n",
    "\n",
    "        train_val_maj = np.abs(np.abs(train_val_maj) - test_val_cM * correct_size_ones)\n",
    "        train_val_maj = np.nan_to_num(np.reciprocal(train_val_maj))\n",
    "        \n",
    "        #train_val_min = np.abs(train_val_min - test_val_cM * correct_size_ones)\n",
    "        #train_val_min = np.abs(train_val_min - np.max(train_val_min))\n",
    "\n",
    "        train_val_min = np.abs(np.abs(train_val_min) - test_val_cM * correct_size_ones)\n",
    "        train_val_min = np.nan_to_num(np.reciprocal(train_val_min))\n",
    "\n",
    "        #train_val_het = np.abs(train_val_het - test_val_cM * correct_size_ones)\n",
    "        #train_val_het = np.abs(train_val_het - np.max(train_val_het))\n",
    "\n",
    "        train_val_het = np.abs(np.abs(train_val_het) - test_val_cM * correct_size_ones)\n",
    "        train_val_het = np.nan_to_num(np.reciprocal(train_val_het))\n",
    "\n",
    "        \n",
    "        #for val in range(len(train_val_maj)):\n",
    "        #    train_val_maj[val] = 1.0 / np.abs(test_val_cM - train_val_maj[val])\n",
    "        #    train_val_min[val] = 1.0 / np.abs(test_val_cM - train_val_min[val])\n",
    "        #    train_val_het[val] = 1.0 / np.abs(test_val_cM - train_val_het[val])\n",
    "\n",
    "        \n",
    "        #add labels to label vector\n",
    "        mega_labels_maj[((SNP-offset) * people_training) : ((SNP-offset + 1) * people_training)] = maj_homo_train_T[:,SNP]\n",
    "        mega_labels_min[((SNP-offset) * people_training) : ((SNP-offset + 1) * people_training)] = min_homo_train_T[:,SNP]\n",
    "        mega_labels_het[((SNP-offset) * people_training) : ((SNP-offset + 1) * people_training)] = hetero_train_T[:,SNP]\n",
    "        \n",
    "        #add to the mega data\n",
    "        mega_training_data_maj[(SNP-offset) * people_training : (SNP-offset + 1) * people_training, :] = train_val_maj\n",
    "        mega_training_data_min[(SNP-offset) * people_training : (SNP-offset + 1) * people_training, :] = train_val_min\n",
    "        mega_training_data_het[(SNP-offset) * people_training : (SNP-offset + 1) * people_training, :] = train_val_het\n",
    "        print(np.max(mega_training_data_maj[(SNP-offset) * people_training : (SNP-offset + 1) * people_training, :]))\n",
    "\n",
    "\n",
    "    snpX_maj = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "    snpX_min = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "    snpX_het = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "\n",
    "    snpX_maj.fit(mega_training_data_maj, mega_labels_maj)\n",
    "    snpX_min.fit(mega_training_data_min, mega_labels_min)\n",
    "    snpX_het.fit(mega_training_data_het, mega_labels_het)\n",
    "    t2 = time.time()\n",
    "    total_time = t2-t1\n",
    "    return total_time, snpX_maj, snpX_min, snpX_het"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving time vals\n",
    "time_off_25_thin_0p1 = 2562.2098598480225 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REWRITE THIS\n",
    "import math\n",
    "def testing_faster(offset, thin_var, snpX_maj, snpX_min, snpX_het):\n",
    "    t1 = time.time()\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    #create indices that will be \"missing\"\n",
    "    idx_imputing = np.random.choice(int((maj_homo_test_T.shape[1])), int((maj_homo_test_T.shape[1]) * thin_var), replace = False)\n",
    "    missing_idx = np.sort(idx_imputing)\n",
    "    missing_idx = list(missing_idx)\n",
    "    missing_indices = []\n",
    "    for x in missing_idx:\n",
    "        if (x < offset):\n",
    "            missing_idx.remove(x)\n",
    "        elif (x > int((maj_homo_test_T.shape[1])) - offset - 1):\n",
    "            missing_idx.remove(x)\n",
    "        else:\n",
    "            missing_indices.append(x)    \n",
    "    kept_indices = [row for row in range(maj_homo_test_T.shape[1]) if row not in missing_indices]\n",
    "    \n",
    "    H_scores_final = []\n",
    "    \n",
    "    maj_test_scaled = maj_homo_test_T +1\n",
    "    min_test_scaled = min_homo_test_T +1\n",
    "    het_test_scaled = hetero_test_T +1    \n",
    "    \n",
    "    maj_train_scaled = maj_test_scaled * cM_dist\n",
    "    min_train_scaled = min_test_scaled * cM_dist\n",
    "    het_train_scaled = het_test_scaled * cM_dist\n",
    "    \n",
    "    correct_size_ones = np.ones((maj_test_scaled.shape[0], 2*offset))\n",
    "    \n",
    "    for SNP in missing_indices:\n",
    "\n",
    "        #create correct labels\n",
    "        test_label_maj = maj_homo_test_T[:,SNP]\n",
    "        test_label_min = min_homo_test_T[:,SNP]\n",
    "        test_label_het = hetero_test_T[:,SNP]\n",
    "\n",
    "        test_val_cM = cM_dist[SNP]\n",
    "\n",
    "        #calculate the data\n",
    "        below_start_offset = SNP - 1\n",
    "        while below_start_offset not in kept_indices:\n",
    "            below_start_offset -= 1\n",
    "        below_index = kept_indices.index(below_start_offset)\n",
    "\n",
    "        above_start_offset = SNP + 1\n",
    "        while above_start_offset not in kept_indices:\n",
    "            above_start_offset += 1\n",
    "        above_index = kept_indices.index(above_start_offset)\n",
    "        \n",
    "        idx_range = np.r_[kept_indices[(below_index-offset+1):below_index+1],\n",
    "             kept_indices[above_index:above_index+offset]]\n",
    "        idx_range_int = [int(i) for i in idx_range]\n",
    "\n",
    "        test_val_maj = maj_test_scaled[:,idx_range_int]\n",
    "        test_val_min = min_test_scaled[:,idx_range_int]\n",
    "        test_val_het = het_test_scaled[:,idx_range_int]\n",
    "        \n",
    "        #test_val_maj = np.abs(test_val_maj - test_val_cM * correct_size_ones)\n",
    "        #test_val_maj = np.abs(test_val_maj - np.max(test_val_maj))\n",
    "        \n",
    "        test_val_maj = np.abs(np.abs(test_val_maj) - test_val_cM * correct_size_ones)\n",
    "        test_val_maj = np.nan_to_num(np.reciprocal(test_val_maj))\n",
    "        \n",
    "        #test_val_min = np.abs(test_val_min - test_val_cM * correct_size_ones)\n",
    "        #test_val_min = np.abs(test_val_min - np.max(test_val_min))\n",
    "\n",
    "        test_val_min = np.abs(np.abs(test_val_min) - test_val_cM * correct_size_ones)\n",
    "        test_val_min = np.nan_to_num(np.reciprocal(test_val_min))\n",
    "\n",
    "        #test_val_het = np.abs(test_val_het - test_val_cM * correct_size_ones)\n",
    "        #test_val_het = np.abs(test_val_het - np.max(test_val_het))\n",
    "        \n",
    "        test_val_het = np.abs(np.abs(test_val_het) - test_val_cM * correct_size_ones)\n",
    "        test_val_het = np.nan_to_num(np.reciprocal(test_val_het))\n",
    "\n",
    "        \n",
    "        \n",
    "        predict_maj = snpX_maj.predict(test_val_maj)\n",
    "        predict_min = snpX_min.predict(test_val_min)\n",
    "        predict_het = snpX_het.predict(test_val_het)\n",
    "\n",
    "\n",
    "        #Accuracy for Hellinger score\n",
    "        probs_maj = snpX_maj.predict_proba(test_val_maj)\n",
    "        probs_min = snpX_min.predict_proba(test_val_min)\n",
    "        probs_het = snpX_het.predict_proba(test_val_het)\n",
    "\n",
    "        #Normalize for Hellinger score\n",
    "        #if any are all one dimension rather than two, means it is never that. Add a second col anyway\n",
    "        '''probs_het = np.zeros((probs_hetero.shape[0], 2))\n",
    "        if probs_hetero.shape[1] == 1:\n",
    "            for i in range((probs_hetero.shape[0])):\n",
    "                probs_het[i][0] = probs_hetero[i]\n",
    "        else:\n",
    "            probs_het = probs_hetero\n",
    "\n",
    "        probs_maj = np.zeros((probs_maj_homo.shape[0], 2))\n",
    "        if probs_maj_homo.shape[1] == 1:\n",
    "            for i in range((probs_maj_homo.shape[0])):\n",
    "                probs_maj[i][0] = probs_maj_homo[i]\n",
    "        else:\n",
    "            probs_maj = probs_maj_homo\n",
    "\n",
    "\n",
    "        probs_min = np.zeros((probs_min_homo.shape[0], 2))\n",
    "        if probs_min_homo.shape[1] == 1:\n",
    "            for i in range((probs_min_homo.shape[0])):\n",
    "                probs_min[i][0] = probs_min_homo[i]\n",
    "        else:\n",
    "            probs_min = probs_min_homo'''\n",
    "\n",
    "\n",
    "        #HERE --> create \"normalized\" distributions for the three\n",
    "        prob_dist_predict = np.zeros(( probs_min.shape[0], 3))\n",
    "        for person in range(probs_min.shape[0]):\n",
    "            total = probs_min[person][1] + probs_maj[person][1] + probs_het[person][1]\n",
    "            prob_dist_predict[person][0] = probs_maj[person][1] / total\n",
    "            prob_dist_predict[person][1] = probs_het[person][1] / total\n",
    "            prob_dist_predict[person][2] = probs_min[person][1] / total\n",
    "\n",
    "        #calculate actual prob_dist_actual\n",
    "        prob_dist_actual = np.zeros((probs_min.shape[0], 3))\n",
    "        H_score = []\n",
    "\n",
    "        for person in range((probs_min.shape[0])):\n",
    "            sum_total = test_label_maj[person] + test_label_het[person] +\\\n",
    "            test_label_min[person]\n",
    "            prob_dist_actual[person][0] = test_label_maj[person] / sum_total\n",
    "            prob_dist_actual[person][1] = test_label_het[person] / sum_total\n",
    "            prob_dist_actual[person][2] = test_label_min[person] / sum_total\n",
    "\n",
    "            sqrt_sum = math.sqrt(prob_dist_actual[person][0] * prob_dist_predict[person][0]) + \\\n",
    "                math.sqrt(prob_dist_actual[person][1] * prob_dist_predict[person][1]) + \\\n",
    "                math.sqrt(prob_dist_actual[person][2] * prob_dist_predict[person][2])\n",
    "            if sqrt_sum > 1.0:\n",
    "                sqrt_sum = 1.0\n",
    "            H_score.append(1 - math.sqrt(1-sqrt_sum))\n",
    "        H_scores_final.append(H_score)\n",
    "    t2 = time.time()\n",
    "    total_time = t2-t1\n",
    "\n",
    "    return H_scores_final, total_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n",
      "44871.803501742055\n",
      "44871.803501742055\n",
      "4951\n",
      "44871.803501742055\n",
      "44871.803501742055\n",
      "4952\n",
      "28794.73586140491\n",
      "28794.73586140491\n",
      "4953\n",
      "28794.73586140491\n",
      "28794.73586140491\n",
      "4954\n",
      "23718.62500279407\n",
      "23718.62500279407\n",
      "4955\n",
      "11050.402654547572\n",
      "11050.402654547572\n",
      "4956\n",
      "11050.4026546018\n",
      "11050.4026546018\n",
      "4957\n",
      "19931.65435726536\n",
      "19931.65435726536\n",
      "4958\n",
      "31656.158405072492\n",
      "31656.158405072492\n",
      "4959\n",
      "31656.158405072492\n",
      "31656.158405072492\n",
      "4960\n",
      "71428.57142765033\n",
      "71428.57142765033\n",
      "4961\n",
      "1595227.0796763485\n",
      "1595227.0796763485\n",
      "4962\n",
      "71428.57142765033\n",
      "71428.57142765033\n",
      "4963\n",
      "158687.84197122647\n",
      "158687.84197122647\n",
      "4964\n",
      "59507.976151708244\n",
      "59507.976151708244\n",
      "4965\n",
      "233174.153110585\n",
      "233174.153110585\n",
      "4966\n",
      "233174.153110585\n",
      "233174.153110585\n",
      "4967\n",
      "95238.09524089513\n",
      "95238.09524089513\n",
      "4968\n",
      "95238.09523686711\n",
      "95238.09523686711\n",
      "4969\n",
      "76177.5912929862\n",
      "76177.5912929862\n",
      "4970\n",
      "76177.5912929862\n",
      "76177.5912929862\n",
      "4971\n",
      "525000.5251370175\n",
      "525000.5251370175\n",
      "4972\n",
      "525000.5251370175\n",
      "525000.5251370175\n",
      "4973\n",
      "323076.7242285458\n",
      "323076.7242285458\n",
      "4974\n",
      "55555.555556013984\n",
      "55555.555556013984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-37846ac45cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mthin_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtime_off25_thin0p1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnpX_maj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnpX_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnpX_het\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_faster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthin_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mH_scores_off25_thin0p1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_off25_thin0p1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting_faster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthin_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnpX_maj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnpX_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnpX_het\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-4aa01bfc9f30>\u001b[0m in \u001b[0;36mtraining_faster\u001b[0;34m(offset, thin_var)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0msnpX_maj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmega_training_data_maj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmega_labels_maj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0msnpX_min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmega_training_data_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmega_labels_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0msnpX_het\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmega_training_data_het\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmega_labels_het\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generate_sample_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0msample_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msample_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_generate_sample_indices\u001b[0;34m(random_state, n_samples)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m\"\"\"Private function used to _parallel_build_trees function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mrandom_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0msample_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msample_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "offset = 25\n",
    "thin_var = 0.1\n",
    "time_off25_thin0p1_train, snpX_maj, snpX_min, snpX_het = training_faster(offset = offset, thin_var = 0.1)\n",
    "H_scores_off25_thin0p1, time_off25_thin0p1_test = testing_faster(offset, thin_var, snpX_maj, snpX_min, snpX_het)\n",
    "\n",
    "flat_H_off25_thin0p1 = [item for sublist in H_scores_off25_thin0p1 for item in sublist]\n",
    "\n",
    "np.savetxt(\"H_scores_off25_thin0p1_RF\", flat_H_off25_thin0p1, delimiter = ' ')\n",
    "print(time_off25_thin0p1_train, time_off25_thin0p1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3456\n",
      "5641218.516941938\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "np.random.seed(0)\n",
    "\n",
    "#create indices that will be \"missing\"\n",
    "idx_imputing = np.random.choice(int((maj_homo_train_T.shape[1])), int((maj_homo_train_T.shape[1]) * thin_var), replace = False)\n",
    "missing_idx = np.sort(idx_imputing)\n",
    "missing_idx = list(missing_idx)\n",
    "missing_indices = []\n",
    "for x in missing_idx:\n",
    "    if (x < offset):\n",
    "        missing_idx.remove(x)\n",
    "    elif (x > int((maj_homo_train_T.shape[1])) - offset - 1):\n",
    "        missing_idx.remove(x)\n",
    "    else:\n",
    "        missing_indices.append(x)    \n",
    "kept_indices = [row for row in range(maj_homo_train_T.shape[1]) if row not in missing_indices]\n",
    "\n",
    "\n",
    "imputable_maj_train = maj_homo_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "imputable_min_train = min_homo_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "imputable_het_train = hetero_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "\n",
    "imputable_snps = imputable_maj_train[1]\n",
    "mega_training_data_maj = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "mega_training_data_min = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "mega_training_data_het = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "mega_labels_maj = np.zeros((len(imputable_snps) * people_training))\n",
    "mega_labels_min = np.zeros((len(imputable_snps) * people_training))\n",
    "mega_labels_het = np.zeros((len(imputable_snps) * people_training))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "maj_train_scaled = (((((maj_homo_train_T * 2)+1)*0.5)-1)*2)\n",
    "min_train_scaled = (((((min_homo_train_T * 2)+1)*0.5)-1)*2)\n",
    "het_train_scaled = (((((hetero_train_T * 2)+1)*0.5)-1)*2)\n",
    "\n",
    "#maj_train_scaled = maj_homo_train_T +1\n",
    "#min_train_scaled = min_homo_train_T +1\n",
    "#het_train_scaled = hetero_train_T +1    \n",
    "\n",
    "maj_train_scaled = maj_train_scaled * cM_dist\n",
    "min_train_scaled = min_train_scaled * cM_dist\n",
    "het_train_scaled = het_train_scaled * cM_dist\n",
    "\n",
    "correct_size_ones = np.ones((maj_train_scaled.shape[0], 2*offset))\n",
    "\n",
    "#for SNP in range(offset, (maj_homo_train_T.shape[1] - offset)): #offset to ncol maj_homo_train_T-offset +1\n",
    "\n",
    "SNP = 3456\n",
    "print(SNP)\n",
    "#the SNP being imputed - the label\n",
    "test_val_maj = maj_homo_train_T[:,SNP]\n",
    "test_val_min = min_homo_train_T[:,SNP]\n",
    "test_val_het = hetero_train_T[:,SNP]\n",
    "\n",
    "test_val_cM = cM_dist[SNP]\n",
    "\n",
    "#calculate the data\n",
    "below_start_offset = SNP - 1\n",
    "while below_start_offset not in kept_indices:\n",
    "    below_start_offset -= 1\n",
    "below_index = kept_indices.index(below_start_offset)\n",
    "\n",
    "above_start_offset = SNP + 1\n",
    "while above_start_offset not in kept_indices:\n",
    "    above_start_offset += 1\n",
    "above_index = kept_indices.index(above_start_offset)\n",
    "\n",
    "idx_range = np.r_[kept_indices[(below_index-offset+1):below_index+1],\n",
    "     kept_indices[above_index:above_index+offset]]\n",
    "idx_range_int = [int(i) for i in idx_range]\n",
    "\n",
    "train_val_maj = maj_train_scaled[:,idx_range_int]\n",
    "train_val_min = min_train_scaled[:,idx_range_int]\n",
    "train_val_het = het_train_scaled[:,idx_range_int]\n",
    "\n",
    "#train_val_maj = np.abs(train_val_maj - test_val_cM * correct_size_ones)\n",
    "#train_val_maj = np.abs(train_val_maj - np.max(train_val_maj))\n",
    "\n",
    "train_val_maj = np.abs(np.abs(train_val_maj) - test_val_cM * correct_size_ones)\n",
    "train_val_maj = np.reciprocal(train_val_maj)\n",
    "\n",
    "#train_val_min = np.abs(train_val_min - test_val_cM * correct_size_ones)\n",
    "#train_val_min = np.abs(train_val_min - np.max(train_val_min))\n",
    "\n",
    "train_val_min = np.abs(np.abs(train_val_min) - test_val_cM * correct_size_ones)\n",
    "train_val_min = np.reciprocal(train_val_min)\n",
    "\n",
    "#train_val_het = np.abs(train_val_het - test_val_cM * correct_size_ones)\n",
    "#train_val_het = np.abs(train_val_het - np.max(train_val_het))\n",
    "\n",
    "train_val_het = np.abs(np.abs(train_val_het) - test_val_cM * correct_size_ones)\n",
    "train_val_het = np.reciprocal(train_val_het)\n",
    "\n",
    "print(np.max(train_val_maj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#np.savetxt(\"H_data_50%mask_25offset_RF\", flat_H_offset25_mask, delimiter = ' ')\n",
    "#print(time_off25_thin0p1_train)\n",
    "#2636.1913940906525\n",
    "\n",
    "print(np.max(mega_training_data_maj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.83181881904602\n"
     ]
    }
   ],
   "source": [
    "flat_H_off25_thin0p1 = [item for sublist in H_scores_off25_thin0p1 for item in sublist]\n",
    "\n",
    "np.savetxt(\"H_scores_off25_thin0p1_RF\", flat_H_off25_thin0p1, delimiter = ' ')\n",
    "print(time_off25_thin0p1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "#ax.set_xticks([1, 2, 3, 4, 5, 6, 7])\n",
    "ax.set_xticklabels(['50%, offset25'])\n",
    "ax.boxplot(flat_H_off25_thin0p1, showfliers=True)\n",
    "plt.title(\"Hellinger Scores\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRETTY SURE I CAN STOP HERE\n",
    "#Transpose the matrices so they are people # rows and SNP # columns\n",
    "maj_homo_train_T = maj_homozygous_train.transpose()\n",
    "hetero_train_T = heterozygous_train.transpose()\n",
    "min_homo_train_T = min_homozygous_train.transpose()\n",
    "\n",
    "label_maj_homo = np.zeros((maj_homo_train_T.shape[0]))\n",
    "label_min_homo = np.zeros((min_homo_train_T.shape[0]))\n",
    "label_hetero = np.zeros((hetero_train_T.shape[0]))\n",
    "\n",
    "for i in range(maj_homo_train_T.shape[0]):\n",
    "    label_maj_homo[i] =  maj_homo_train_T[i][idx_snp]\n",
    "    label_min_homo[i] = min_homo_train_T[i][idx_snp]\n",
    "    label_hetero[i] = hetero_train_T[i][idx_snp]\n",
    "\n",
    "label_maj_homo_cM = cM_map[idx_snp][1]\n",
    "new_training_scaled_cM = np.zeros((people_training, 2*offset))\n",
    "new_training_scaled_cM_min = np.zeros((people_training, 2*offset))\n",
    "new_training_scaled_cM_hetero = np.zeros((people_training, 2*offset))\n",
    "\n",
    "diffs = np.zeros((people_training, 2*offset))\n",
    "\n",
    "#creates models\n",
    "diff_val = 0.001\n",
    "#for the 2*offset SNPs being used as data\n",
    "for cMpos in range(2*offset):\n",
    "    for person in range(people_training):\n",
    "        #need to account for not using the actual SNP being imputed\n",
    "        if cMpos < offset:\n",
    "            #calculate cM pos difference between SNP being imputed + reference SNP\n",
    "            diff = np.abs(label_maj_homo_cM - cM_map[idx_snp - offset+ cMpos][1])\n",
    "            diffs[person][cMpos] = diff\n",
    "            #if difference is zero, they have same cM pos position and should stick together\n",
    "            #this only happens at the very first few SNPs (probably)\n",
    "            if diff == 0:  \n",
    "                diff = diff_val #can change this value\n",
    "            \n",
    "            unscaled_data = maj_homo_train_T[person][idx_snp - offset + cMpos]\n",
    "            unscaled_data_min = min_homo_train_T[person][idx_snp - offset + cMpos]\n",
    "            unscaled_data_hetero = hetero_train_T[person][idx_snp - offset + cMpos]\n",
    "\n",
    "            #set 0 vals to -1 (so we don't just set ~1/2+ of our data to 0)\n",
    "            if unscaled_data == 0:\n",
    "                unscaled_data = -1\n",
    "            #new training data is scaled by reciprocal of difference\n",
    "            #so close SNPs have higher impact on imputed val\n",
    "            new_training_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "            new_training_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "            new_training_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "        else:\n",
    "            #print(\"cMpos\", cMpos, \"cMpos Dist\", cM_map[idx_snp - offset+ cMpos +1][1])\n",
    "            diff = np.abs(label_maj_homo_cM - cM_map[idx_snp - offset+ cMpos +1][1])\n",
    "            diffs[person][cMpos] = diff\n",
    "\n",
    "            if diff == 0:  \n",
    "                diff = diff_val \n",
    "            unscaled_data = maj_homo_train_T[person][idx_snp - offset+ cMpos + 1]\n",
    "            unscaled_data_min = min_homo_train_T[person][idx_snp - offset + cMpos + 1]\n",
    "            unscaled_data_hetero = hetero_train_T[person][idx_snp - offset + cMpos + 1]\n",
    "\n",
    "            #set 0 vals to -1 (so we don't just set ~1/2+ of our data to 0)\n",
    "            if unscaled_data == 0:\n",
    "                unscaled_data = -1\n",
    "            #new training data is scaled by reciprocal of difference\n",
    "            #so close SNPs have higher impact on imputed val\n",
    "            new_training_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "            new_training_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "            new_training_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "\n",
    "snpX_maj = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "snpX_min = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "snpX_hetero = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "\n",
    "snpX_maj.fit(new_training_scaled_cM, label_maj_homo)\n",
    "snpX_min.fit(new_training_scaled_cM_min, label_min_homo)\n",
    "snpX_hetero.fit(new_training_scaled_cM_hetero, label_hetero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run through many times (as we do our model)\n",
    "import math\n",
    "\n",
    "def train_and_test(offset, idx_snp):\n",
    "    #training\n",
    "    label_maj_homo = np.zeros((maj_homo_train_T.shape[0]))\n",
    "    label_min_homo = np.zeros((min_homo_train_T.shape[0]))\n",
    "    label_hetero = np.zeros((hetero_train_T.shape[0]))\n",
    "\n",
    "    for i in range(maj_homo_train_T.shape[0]):\n",
    "        label_maj_homo[i] =  maj_homo_train_T[i][idx_snp]\n",
    "        label_min_homo[i] = min_homo_train_T[i][idx_snp]\n",
    "        label_hetero[i] = hetero_train_T[i][idx_snp]\n",
    "\n",
    "    label_maj_homo_cM = cM_map[idx_snp][1]\n",
    "    new_training_scaled_cM = np.zeros((people_training, 2*offset))\n",
    "    new_training_scaled_cM_min = np.zeros((people_training, 2*offset))\n",
    "    new_training_scaled_cM_hetero = np.zeros((people_training, 2*offset))\n",
    "\n",
    "    diffs = np.zeros((people_training, 2*offset))\n",
    "\n",
    "    #creates models\n",
    "    diff_val = 0.001\n",
    "    #for the 2*offset SNPs being used as data\n",
    "    for cMpos in range(2*offset):\n",
    "        for person in range(people_training):\n",
    "            #need to account for not using the actual SNP being imputed\n",
    "            if cMpos < offset:\n",
    "                #calculate cM pos difference between SNP being imputed + reference SNP\n",
    "                diff = np.abs(label_maj_homo_cM - cM_map[idx_snp - offset+ cMpos][1])\n",
    "                diffs[person][cMpos] = diff\n",
    "                #if difference is zero, they have same cM pos position and should stick together\n",
    "                #this only happens at the very first few SNPs (probably)\n",
    "                if diff == 0:  \n",
    "                    diff = diff_val #can change this value\n",
    "\n",
    "                unscaled_data = maj_homo_train_T[person][idx_snp - offset + cMpos]\n",
    "                unscaled_data_min = min_homo_train_T[person][idx_snp - offset + cMpos]\n",
    "                unscaled_data_hetero = hetero_train_T[person][idx_snp - offset + cMpos]\n",
    "\n",
    "                #set 0 vals to -1 (so we don't just set ~1/2+ of our data to 0)\n",
    "                if unscaled_data == 0:\n",
    "                    unscaled_data = -1\n",
    "                #new training data is scaled by reciprocal of difference\n",
    "                #so close SNPs have higher impact on imputed val\n",
    "                new_training_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "                new_training_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "                new_training_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "            else:\n",
    "                #print(\"cMpos\", cMpos, \"cMpos Dist\", cM_map[idx_snp - offset+ cMpos +1][1])\n",
    "                diff = np.abs(label_maj_homo_cM - cM_map[idx_snp - offset+ cMpos +1][1])\n",
    "                diffs[person][cMpos] = diff\n",
    "\n",
    "                if diff == 0:  \n",
    "                    diff = diff_val \n",
    "                unscaled_data = maj_homo_train_T[person][idx_snp - offset+ cMpos + 1]\n",
    "                unscaled_data_min = min_homo_train_T[person][idx_snp - offset + cMpos + 1]\n",
    "                unscaled_data_hetero = hetero_train_T[person][idx_snp - offset + cMpos + 1]\n",
    "\n",
    "                #set 0 vals to -1 (so we don't just set ~1/2+ of our data to 0)\n",
    "                if unscaled_data == 0:\n",
    "                    unscaled_data = -1\n",
    "                #new training data is scaled by reciprocal of difference\n",
    "                #so close SNPs have higher impact on imputed val\n",
    "                new_training_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "                new_training_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "                new_training_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "\n",
    "    snpX_maj = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "    snpX_min = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "    snpX_hetero = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "\n",
    "    snpX_maj.fit(new_training_scaled_cM, label_maj_homo)\n",
    "    snpX_min.fit(new_training_scaled_cM_min, label_min_homo)\n",
    "    snpX_hetero.fit(new_training_scaled_cM_hetero, label_hetero)\n",
    "    \n",
    "    #testing\n",
    "    label_maj_homo_test = np.zeros((maj_homo_test_T.shape[0]))\n",
    "    label_min_homo_test = np.zeros((min_homo_test_T.shape[0]))\n",
    "    label_hetero_test = np.zeros((hetero_test_T.shape[0]))\n",
    "\n",
    "    for i in range(maj_homo_test_T.shape[0]):\n",
    "        label_maj_homo_test[i] =  maj_homo_test_T[i][idx_snp]\n",
    "        label_min_homo_test[i] =  min_homo_test_T[i][idx_snp]\n",
    "        label_hetero_test[i] =  hetero_test_T[i][idx_snp]\n",
    "\n",
    "    new_testing_scaled_cM = np.zeros((people, 2*offset))\n",
    "    new_testing_scaled_cM_min = np.zeros((people, 2*offset))\n",
    "    new_testing_scaled_cM_hetero = np.zeros((people, 2*offset))\n",
    "\n",
    "    label_maj_homo_cM = cM_map[idx_snp][1]\n",
    "\n",
    "    for cMpos in range(2*offset):\n",
    "        for person in range(people):\n",
    "            if cMpos < offset:\n",
    "                current_idx = idx_snp - cMpos - 1\n",
    "                if current_idx in missing_indices:\n",
    "                    current_idx -= 1\n",
    "                #calculate cM pos difference between SNP being imputed + reference SNP\n",
    "                diff = np.abs(label_maj_homo_cM - cM_map[current_idx][1])\n",
    "\n",
    "                if diff == 0:  \n",
    "                    diff = diff_val #can change this value\n",
    "\n",
    "                unscaled_data = maj_homo_test_T[person][current_idx]\n",
    "                unscaled_data_min = min_homo_test_T[person][current_idx]\n",
    "                unscaled_data_hetero = hetero_test_T[person][current_idx]\n",
    "\n",
    "                if unscaled_data == 0:\n",
    "                    unscaled_data = -1\n",
    "                new_testing_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "                new_testing_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "                new_testing_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "            else:\n",
    "                current_idx = idx_snp - offset+ cMpos +1\n",
    "                if current_idx in missing_indices:\n",
    "                    current_idx += 1\n",
    "                diff = np.abs(label_maj_homo_cM - cM_map[current_idx][1])\n",
    "                if diff == 0:  \n",
    "                    diff = diff_val \n",
    "                unscaled_data = maj_homo_test_T[person][current_idx]\n",
    "                unscaled_data_min = min_homo_test_T[person][current_idx]\n",
    "                unscaled_data_hetero = hetero_test_T[person][current_idx]\n",
    "\n",
    "                if unscaled_data == 0:\n",
    "                    unscaled_data = -1\n",
    "                new_testing_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "                new_testing_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "                new_testing_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "    predict_label_maj_100 = snpX_maj.predict(new_testing_scaled_cM)\n",
    "    predict_label_min_100 = snpX_min.predict(new_testing_scaled_cM_min)\n",
    "    predict_label_hetero_100 = snpX_hetero.predict(new_testing_scaled_cM_hetero)\n",
    "    \n",
    "    \n",
    "    #Accuracy for Hellinger score\n",
    "    probs_maj_homo = snpX_maj.predict_proba(new_testing_scaled_cM)\n",
    "    probs_min_homo = snpX_min.predict_proba(new_testing_scaled_cM_min)\n",
    "    probs_hetero = snpX_hetero.predict_proba(new_testing_scaled_cM_hetero)\n",
    "\n",
    "    #Normalize for Hellinger score\n",
    "    #if any are all one dimension rather than two, means it is never that. Add a second col anyway\n",
    "    probs_het = np.zeros((probs_hetero.shape[0], 2))\n",
    "    if probs_hetero.shape[1] == 1:\n",
    "        for i in range((probs_hetero.shape[0])):\n",
    "            probs_het[i][0] = probs_hetero[i]\n",
    "    else:\n",
    "        probs_het = probs_hetero\n",
    "\n",
    "    probs_maj = np.zeros((probs_maj_homo.shape[0], 2))\n",
    "    if probs_maj_homo.shape[1] == 1:\n",
    "        for i in range((probs_maj_homo.shape[0])):\n",
    "            probs_maj[i][0] = probs_maj_homo[i]\n",
    "    else:\n",
    "        probs_maj = probs_maj_homo\n",
    "\n",
    "\n",
    "    probs_min = np.zeros((probs_min_homo.shape[0], 2))\n",
    "    if probs_min_homo.shape[1] == 1:\n",
    "        for i in range((probs_min_homo.shape[0])):\n",
    "            probs_min[i][0] = probs_min_homo[i]\n",
    "    else:\n",
    "        probs_min = probs_min_homo\n",
    "\n",
    "        \n",
    "    #HERE --> create \"normalized\" distributions for the three\n",
    "    prob_dist_predict = np.zeros(( probs_min.shape[0], 3))\n",
    "    for person in range(probs_min.shape[0]):\n",
    "        total = probs_min[person][1] + probs_maj[person][1] + probs_het[person][1]\n",
    "        prob_dist_predict[person][0] = probs_maj[person][1] / total\n",
    "        prob_dist_predict[person][1] = probs_het[person][1] / total\n",
    "        prob_dist_predict[person][2] = probs_min[person][1] / total\n",
    "\n",
    "    #calculate actual prob_dist_actual\n",
    "    prob_dist_actual = np.zeros((probs_min.shape[0], 3))\n",
    "    H_score = []\n",
    "\n",
    "    for person in range((probs_min.shape[0])):\n",
    "        sum_total = label_hetero_test[person] + label_maj_homo_test[person] +\\\n",
    "        label_min_homo_test[person]\n",
    "        prob_dist_actual[person][0] = label_maj_homo_test[person] / sum_total\n",
    "        prob_dist_actual[person][1] = label_hetero_test[person] / sum_total\n",
    "        prob_dist_actual[person][2] = label_min_homo_test[person] / sum_total\n",
    "    \n",
    "        sqrt_sum = math.sqrt(prob_dist_actual[person][0] * prob_dist_predict[person][0]) + \\\n",
    "            math.sqrt(prob_dist_actual[person][1] * prob_dist_predict[person][1]) + \\\n",
    "            math.sqrt(prob_dist_actual[person][2] * prob_dist_predict[person][2])\n",
    "        if sqrt_sum > 1.0:\n",
    "            sqrt_sum = 1.0\n",
    "        H_score.append(1 - math.sqrt(1-sqrt_sum))\n",
    "    return H_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS\n",
    "\n",
    "def training(offset):\n",
    "    #for a given offset, train this model on all data\n",
    "    \n",
    "    #we are only going to train when we're offset or more away from the ends\n",
    "    imputable_maj_train = maj_homo_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "    imputable_min_train = min_homo_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "    imputable_het_train = hetero_train_T[:, offset:maj_homo_train_T.shape[1]-offset]\n",
    "\n",
    "    imputable_snps = imputable_maj_train[1]\n",
    "    mega_training_data_maj = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "    mega_training_data_min = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "    mega_training_data_het = np.zeros((people_training * len(imputable_snps), 2*offset))\n",
    "    mega_labels_maj = np.zeros((len(imputable_snps) * people_training))\n",
    "    mega_labels_min = np.zeros((len(imputable_snps) * people_training))\n",
    "    mega_labels_het = np.zeros((len(imputable_snps) * people_training))\n",
    "\n",
    "                \n",
    "    #from imputable_*_train remove all of the indices that have been thinned out\n",
    "    #call it thinned_*_train\n",
    "    \n",
    "    #then iterate on every snp and multiply by centi-morgans\n",
    "    \n",
    "    for SNP in range(len(imputable_snps)):\n",
    "        print(SNP)\n",
    "        \n",
    "        \n",
    "        #labels vectors are 250 people long\n",
    "        label_maj_homo = np.zeros((imputable_maj_train.shape[0]))\n",
    "        label_min_homo = np.zeros((imputable_min_train.shape[0]))\n",
    "        label_hetero = np.zeros((imputable_het_train.shape[0]))\n",
    "\n",
    "        #generate a label for each person\n",
    "        for person in range(maj_homo_train_T.shape[0]):\n",
    "            label_maj_homo[person] =  imputable_maj_train[person, int(SNP)]\n",
    "            label_min_homo[person] = imputable_min_train[person, int(SNP)]\n",
    "            label_hetero[person] = imputable_het_train[person, int(SNP)]\n",
    "\n",
    "        #add labels to label vector\n",
    "        mega_labels_maj[int(SNP * people_training) : int((SNP + 1) * people_training)] = label_maj_homo\n",
    "        mega_labels_min[int(SNP * people_training) : int((SNP + 1) * people_training)] = label_min_homo\n",
    "        mega_labels_het[int(SNP * people_training) : int((SNP + 1) * people_training)] = label_hetero\n",
    "        \n",
    "\n",
    "            \n",
    "        #find cM value for label\n",
    "        label_maj_homo_cM = cM_map[SNP][1]\n",
    "        new_training_scaled_cM = np.zeros((people_training, 2*offset))\n",
    "        new_training_scaled_cM_min = np.zeros((people_training, 2*offset))\n",
    "        new_training_scaled_cM_hetero = np.zeros((people_training, 2*offset))\n",
    "\n",
    "        diffs = np.zeros((people_training, 2*offset))\n",
    "\n",
    "        #creates models\n",
    "        diff_val = 0.001\n",
    "        #for the 2*offset SNPs being used as data\n",
    "        for cMpos in range(2*offset):\n",
    "            for person in range(people_training):\n",
    "                #need to account for not using the actual SNP being imputed\n",
    "                if cMpos < offset:\n",
    "                    current_idx = idx_snp - offset+ cMpos\n",
    "                    \n",
    "                    if current_idx in missing_indices:\n",
    "                        current_idx -= 1\n",
    "\n",
    "                    #calculate cM pos difference between SNP being imputed + reference SNP\n",
    "                    diff = np.abs(label_maj_homo_cM - cM_map[current_idx][1])\n",
    "                    diffs[person][cMpos] = diff\n",
    "\n",
    "                    if diff == 0:  \n",
    "                        diff = diff_val \n",
    "\n",
    "                    unscaled_data = maj_homo_train_T[person][current_idx]\n",
    "                    unscaled_data_min = min_homo_train_T[person][current_idx]\n",
    "                    unscaled_data_hetero = hetero_train_T[person][current_idx]\n",
    "\n",
    "                    if unscaled_data == 0:\n",
    "                        unscaled_data = -1\n",
    "\n",
    "                    new_training_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "                    new_training_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "                    new_training_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "                else:\n",
    "                    current_idx = idx_snp - offset+ cMpos +1\n",
    "                    if current_idx in missing_indices:\n",
    "                        current_idx += 1\n",
    "\n",
    "                    diff = np.abs(label_maj_homo_cM - cM_map[current_idx][1])\n",
    "                    diffs[person][cMpos] = diff\n",
    "\n",
    "                    if diff == 0:  \n",
    "                        diff = diff_val \n",
    "                    unscaled_data = maj_homo_train_T[person][current_idx]\n",
    "                    unscaled_data_min = min_homo_train_T[person][current_idx]\n",
    "                    unscaled_data_hetero = hetero_train_T[person][current_idx]\n",
    "\n",
    "                    #set 0 vals to -1 (so we don't just set ~1/2+ of our data to 0)\n",
    "                    if unscaled_data == 0:\n",
    "                        unscaled_data = -1\n",
    "                    #new training data is scaled by reciprocal of difference\n",
    "                    #so close SNPs have higher impact on imputed val\n",
    "                    new_training_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "                    new_training_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "                    new_training_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "        #add to the mega data\n",
    "        mega_training_data_maj[SNP * people_training : (SNP + 1) * people_training, :] = new_training_scaled_cM\n",
    "        mega_training_data_min[SNP * people_training : (SNP + 1) * people_training, :] = new_training_scaled_cM_min\n",
    "        mega_training_data_het[SNP * people_training : (SNP + 1) * people_training, :] = new_training_scaled_cM_hetero\n",
    "        \n",
    "        \n",
    "        \n",
    "    snpX_maj = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "    snpX_min = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "    snpX_het = RandomForestClassifier(n_estimators = 400, max_depth=6, class_weight = \"balanced\")\n",
    "\n",
    "    snpX_maj.fit(mega_training_data_maj, mega_labels_maj)\n",
    "    snpX_min.fit(mega_training_data_min, mega_labels_min)\n",
    "    snpX_het.fit(mega_training_data_het, mega_labels_het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REWRITE THIS\n",
    "def testing(missing_indices):\n",
    "    H_scores_final = []\n",
    "    for idx_snp in missing_indices:\n",
    "        label_maj_homo_test = np.zeros((maj_homo_test_T.shape[0]))\n",
    "        label_min_homo_test = np.zeros((min_homo_test_T.shape[0]))\n",
    "        label_hetero_test = np.zeros((hetero_test_T.shape[0]))\n",
    "\n",
    "        for i in range(maj_homo_test_T.shape[0]):\n",
    "            label_maj_homo_test[i] =  maj_homo_test_T[i][idx_snp]\n",
    "            label_min_homo_test[i] =  min_homo_test_T[i][idx_snp]\n",
    "            label_hetero_test[i] =  hetero_test_T[i][idx_snp]\n",
    "\n",
    "        new_testing_scaled_cM = np.zeros((people, 2*offset))\n",
    "        new_testing_scaled_cM_min = np.zeros((people, 2*offset))\n",
    "        new_testing_scaled_cM_hetero = np.zeros((people, 2*offset))\n",
    "\n",
    "        label_maj_homo_cM = cM_map[idx_snp][1]\n",
    "\n",
    "        for cMpos in range(2*offset):\n",
    "            for person in range(people):\n",
    "                if cMpos < offset:\n",
    "                    current_idx = idx_snp - cMpos - 1\n",
    "                    if current_idx in missing_indices:\n",
    "                        current_idx -= 1\n",
    "                    #calculate cM pos difference between SNP being imputed + reference SNP\n",
    "                    diff = np.abs(label_maj_homo_cM - cM_map[current_idx][1])\n",
    "\n",
    "                    if diff == 0:  \n",
    "                        diff = diff_val #can change this value\n",
    "\n",
    "                    unscaled_data = maj_homo_test_T[person][current_idx]\n",
    "                    unscaled_data_min = min_homo_test_T[person][current_idx]\n",
    "                    unscaled_data_hetero = hetero_test_T[person][current_idx]\n",
    "\n",
    "                    if unscaled_data == 0:\n",
    "                        unscaled_data = -1\n",
    "                    new_testing_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "                    new_testing_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "                    new_testing_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "                else:\n",
    "                    current_idx = idx_snp - offset+ cMpos +1\n",
    "                    if current_idx in missing_indices:\n",
    "                        current_idx += 1\n",
    "                    diff = np.abs(label_maj_homo_cM - cM_map[current_idx][1])\n",
    "                    if diff == 0:  \n",
    "                        diff = diff_val \n",
    "                    unscaled_data = maj_homo_test_T[person][current_idx]\n",
    "                    unscaled_data_min = min_homo_test_T[person][current_idx]\n",
    "                    unscaled_data_hetero = hetero_test_T[person][current_idx]\n",
    "\n",
    "                    if unscaled_data == 0:\n",
    "                        unscaled_data = -1\n",
    "                    new_testing_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "                    new_testing_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "                    new_testing_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "        predict_label_maj_100 = snpX_maj.predict(new_testing_scaled_cM)\n",
    "        predict_label_min_100 = snpX_min.predict(new_testing_scaled_cM_min)\n",
    "        predict_label_hetero_100 = snpX_hetero.predict(new_testing_scaled_cM_hetero)\n",
    "\n",
    "\n",
    "        #Accuracy for Hellinger score\n",
    "        probs_maj_homo = snpX_maj.predict_proba(new_testing_scaled_cM)\n",
    "        probs_min_homo = snpX_min.predict_proba(new_testing_scaled_cM_min)\n",
    "        probs_hetero = snpX_hetero.predict_proba(new_testing_scaled_cM_hetero)\n",
    "\n",
    "        #Normalize for Hellinger score\n",
    "        #if any are all one dimension rather than two, means it is never that. Add a second col anyway\n",
    "        probs_het = np.zeros((probs_hetero.shape[0], 2))\n",
    "        if probs_hetero.shape[1] == 1:\n",
    "            for i in range((probs_hetero.shape[0])):\n",
    "                probs_het[i][0] = probs_hetero[i]\n",
    "        else:\n",
    "            probs_het = probs_hetero\n",
    "\n",
    "        probs_maj = np.zeros((probs_maj_homo.shape[0], 2))\n",
    "        if probs_maj_homo.shape[1] == 1:\n",
    "            for i in range((probs_maj_homo.shape[0])):\n",
    "                probs_maj[i][0] = probs_maj_homo[i]\n",
    "        else:\n",
    "            probs_maj = probs_maj_homo\n",
    "\n",
    "\n",
    "        probs_min = np.zeros((probs_min_homo.shape[0], 2))\n",
    "        if probs_min_homo.shape[1] == 1:\n",
    "            for i in range((probs_min_homo.shape[0])):\n",
    "                probs_min[i][0] = probs_min_homo[i]\n",
    "        else:\n",
    "            probs_min = probs_min_homo\n",
    "\n",
    "\n",
    "        #HERE --> create \"normalized\" distributions for the three\n",
    "        prob_dist_predict = np.zeros(( probs_min.shape[0], 3))\n",
    "        for person in range(probs_min.shape[0]):\n",
    "            total = probs_min[person][1] + probs_maj[person][1] + probs_het[person][1]\n",
    "            prob_dist_predict[person][0] = probs_maj[person][1] / total\n",
    "            prob_dist_predict[person][1] = probs_het[person][1] / total\n",
    "            prob_dist_predict[person][2] = probs_min[person][1] / total\n",
    "\n",
    "        #calculate actual prob_dist_actual\n",
    "        prob_dist_actual = np.zeros((probs_min.shape[0], 3))\n",
    "        H_score = []\n",
    "\n",
    "        for person in range((probs_min.shape[0])):\n",
    "            sum_total = label_hetero_test[person] + label_maj_homo_test[person] +\\\n",
    "            label_min_homo_test[person]\n",
    "            prob_dist_actual[person][0] = label_maj_homo_test[person] / sum_total\n",
    "            prob_dist_actual[person][1] = label_hetero_test[person] / sum_total\n",
    "            prob_dist_actual[person][2] = label_min_homo_test[person] / sum_total\n",
    "\n",
    "            sqrt_sum = math.sqrt(prob_dist_actual[person][0] * prob_dist_predict[person][0]) + \\\n",
    "                math.sqrt(prob_dist_actual[person][1] * prob_dist_predict[person][1]) + \\\n",
    "                math.sqrt(prob_dist_actual[person][2] * prob_dist_predict[person][2])\n",
    "            if sqrt_sum > 1.0:\n",
    "                sqrt_sum = 1.0\n",
    "            H_score.append(1 - math.sqrt(1-sqrt_sum))\n",
    "        H_scores_final.append(H_score)\n",
    "    return H_scores_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS SECOND\n",
    "training(offset = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN FIFTH, CHANGE VALUES\n",
    "flat_H_offset25_mask = [item for sublist in H_scores_offset24 for item in sublist]\n",
    "np.savetxt(\"H_data_50%mask_25offset_RF\", flat_H_offset25_mask, delimiter = ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_homo_test_T = maj_homozygous_test.transpose()\n",
    "hetero_test_T = heterozygous_test.transpose()\n",
    "min_homo_test_T = min_homozygous_test.transpose()\n",
    "\n",
    "label_maj_homo_test = np.zeros((maj_homo_test_T.shape[0]))\n",
    "label_min_homo_test = np.zeros((min_homo_test_T.shape[0]))\n",
    "label_hetero_test = np.zeros((hetero_test_T.shape[0]))\n",
    "\n",
    "for i in range(maj_homo_test_T.shape[0]):\n",
    "    label_maj_homo_test[i] =  maj_homo_test_T[i][idx_snp]\n",
    "    label_min_homo_test[i] =  min_homo_test_T[i][idx_snp]\n",
    "    label_hetero_test[i] =  hetero_test_T[i][idx_snp]\n",
    "\n",
    "new_testing_scaled_cM = np.zeros((people, 2*offset))\n",
    "new_testing_scaled_cM_min = np.zeros((people, 2*offset))\n",
    "new_testing_scaled_cM_hetero = np.zeros((people, 2*offset))\n",
    "\n",
    "label_maj_homo_cM = cM_map[idx_snp][1]\n",
    "\n",
    "for cMpos in range(2*offset):\n",
    "    for person in range(people):\n",
    "        if cMpos < offset:\n",
    "            current_idx = idx_snp - cMpos - 1\n",
    "            if current_idx in missing_idx:\n",
    "                current_idx -= 1\n",
    "            #calculate cM pos difference between SNP being imputed + reference SNP\n",
    "            diff = np.abs(label_maj_homo_cM - cM_map[current_idx][1])\n",
    "\n",
    "            if diff == 0:  \n",
    "                diff = diff_val #can change this value\n",
    "            \n",
    "            unscaled_data = maj_homo_test_T[person][current_idx]\n",
    "            unscaled_data_min = min_homo_test_T[person][current_idx]\n",
    "            unscaled_data_hetero = hetero_test_T[person][current_idx]\n",
    "            \n",
    "            if unscaled_data == 0:\n",
    "                unscaled_data = -1\n",
    "            new_testing_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "            new_testing_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "            new_testing_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "        else:\n",
    "            current_idx = idx_snp - offset+ cMpos +1\n",
    "            if current_idx in missing_idx:\n",
    "                current_idx += 1\n",
    "            diff = np.abs(label_maj_homo_cM - cM_map[current_idx][1])\n",
    "            if diff == 0:  \n",
    "                diff = diff_val \n",
    "            unscaled_data = maj_homo_test_T[person][current_idx]\n",
    "            unscaled_data_min = min_homo_test_T[person][current_idx]\n",
    "            unscaled_data_hetero = hetero_test_T[person][current_idx]\n",
    "\n",
    "            if unscaled_data == 0:\n",
    "                unscaled_data = -1\n",
    "            new_testing_scaled_cM[person][cMpos] = unscaled_data * (1.0 / diff )\n",
    "            new_testing_scaled_cM_min[person][cMpos] = unscaled_data_min * (1.0 / diff )\n",
    "            new_testing_scaled_cM_hetero[person][cMpos] = unscaled_data_hetero * (1.0 / diff )\n",
    "\n",
    "predict_label_maj_100 = snpX_maj.predict(new_testing_scaled_cM)\n",
    "predict_label_min_100 = snpX_min.predict(new_testing_scaled_cM_min)\n",
    "predict_label_hetero_100 = snpX_hetero.predict(new_testing_scaled_cM_hetero)\n",
    "\n",
    "\n",
    "\n",
    "#calculate accuracy\n",
    "accuracy = 0\n",
    "for i in range(len(predict_label_maj_100)):\n",
    "    if predict_label_maj_100[i] == label_maj_homo_test[i]:\n",
    "        accuracy += 1\n",
    "acc_score = accuracy / len(predict_label_maj_100)\n",
    "\n",
    "min_accuracy = 0\n",
    "for i in range(len(predict_label_min_100)):\n",
    "    if predict_label_min_100[i] == label_min_homo_test[i]:\n",
    "        min_accuracy += 1\n",
    "acc_score = min_accuracy / len(predict_label_min_100)\n",
    "\n",
    "hetero_accuracy = 0\n",
    "for i in range(len(predict_label_hetero_100)):\n",
    "    if predict_label_hetero_100[i] == label_hetero_test[i]:\n",
    "        hetero_accuracy += 1\n",
    "acc_score = hetero_accuracy / len(predict_label_hetero_100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate probability of labels\n",
    "#gives first probability of 0, then of 1\n",
    "#will want one of these (the 1th column) for use in Hellinger score\n",
    "probs_maj_homo = snpX_maj.predict_proba(new_testing_scaled_cM)\n",
    "probs_min_homo = snpX_min.predict_proba(new_testing_scaled_cM_min)\n",
    "probs_hetero = snpX_hetero.predict_proba(new_testing_scaled_cM_hetero)\n",
    "\n",
    "#Normalize for Hellinger score\n",
    "#if any are all one dimension rather than two, means it is never that. Add a second col anyway\n",
    "probs_het = np.zeros((probs_hetero.shape[0], 2))\n",
    "if probs_hetero.shape[1] == 1:\n",
    "    for i in range((probs_hetero.shape[0])):\n",
    "        probs_het[i][0] = probs_hetero[i]\n",
    "else:\n",
    "    probs_het = probs_hetero\n",
    "    \n",
    "probs_maj = np.zeros((probs_maj_homo.shape[0], 2))\n",
    "if probs_maj_homo.shape[1] == 1:\n",
    "    for i in range((probs_maj_homo.shape[0])):\n",
    "        probs_maj[i][0] = probs_maj_homo[i]\n",
    "else:\n",
    "    probs_maj = probs_maj_homo\n",
    "\n",
    "    \n",
    "probs_min = np.zeros((probs_min_homo.shape[0], 2))\n",
    "if probs_min_homo.shape[1] == 1:\n",
    "    for i in range((probs_min_homo.shape[0])):\n",
    "        probs_min[i][0] = probs_min_homo[i]\n",
    "else:\n",
    "    probs_min = probs_min_homo\n",
    "\n",
    "prob_dist_predict = np.zeros(( probs_min.shape[0], 3))\n",
    "for person in range(probs_min.shape[0]):\n",
    "    total = probs_min[person][1] + probs_maj[person][1] + probs_het[person][1]\n",
    "    prob_dist_predict[person][0] = probs_maj[person][1] / total\n",
    "    prob_dist_predict[person][1] = probs_het[person][1] / total\n",
    "    prob_dist_predict[person][2] = probs_min[person][1] / total\n",
    "\n",
    "#print(prob_dist_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create \"labels\" vector for training\n",
    "#starting with major homozygous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
